{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINI Interview Agent\n",
    "\n",
    "A streamlined agent for conducting MINI (Mini International Neuropsychiatric Interview) assessments with intelligent response analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.98.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.8-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.42-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2025.7.34-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading orjson-3.11.1-cp313-cp313-macosx_15_0_arm64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.72-py3-none-any.whl (442 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading sqlalchemy-2.0.42-cp313-cp313-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading openai-1.98.0-py3-none-any.whl (767 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.7/767.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.10.0-cp313-cp313-macosx_11_0_arm64.whl (318 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading langsmith-0.4.8-py3-none-any.whl (367 kB)\n",
      "Downloading orjson-3.11.1-cp313-cp313-macosx_15_0_arm64.whl (129 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl (633 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m633.4/633.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Downloading regex-2025.7.34-cp313-cp313-macosx_11_0_arm64.whl (285 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: zstandard, urllib3, typing-extensions, tqdm, tenacity, sniffio, regex, PyYAML, orjson, jsonpointer, jiter, idna, h11, distro, charset_normalizer, certifi, annotated-types, typing-inspection, SQLAlchemy, requests, pydantic-core, jsonpatch, httpcore, anyio, tiktoken, requests-toolbelt, pydantic, httpx, openai, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/34\u001b[0m [langchain]34\u001b[0m [langchain]core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PyYAML-6.0.2 SQLAlchemy-2.0.42 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.7.14 charset_normalizer-3.4.2 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.27 langchain-core-0.3.72 langchain-openai-0.3.28 langchain-text-splitters-0.3.9 langsmith-0.4.8 openai-1.98.0 orjson-3.11.1 pydantic-2.11.7 pydantic-core-2.33.2 regex-2025.7.34 requests-2.32.4 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 tiktoken-0.9.0 tqdm-4.67.1 typing-extensions-4.14.1 typing-inspection-0.4.1 urllib3-2.5.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain openai tiktoken langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def get_key():\n",
    "    \"\"\"Get OpenAI API key from environment or user input.\"\"\"\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not api_key:\n",
    "        api_key = getpass.getpass('Enter your OpenAI API key: ')\n",
    "        os.environ['OPENAI_API_KEY'] = api_key\n",
    "    return api_key\n",
    "\n",
    "# Set up API key\n",
    "get_key()\n",
    "print(\"API key configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core interview tools defined successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "@tool\n",
    "def ask_patient(question: str) -> str:\n",
    "    \"\"\"Ask the patient the current MINI question and get their response.\"\"\"\n",
    "    print(f\"Clinician: {question}\")\n",
    "    patient_response = input(\"Patient: \")\n",
    "    print(f\"Patient response: {patient_response}\")  # Print patient answer\n",
    "    return patient_response\n",
    "\n",
    "def analyze_response(response: str, question_context: str) -> str:\n",
    "    \"\"\"Use LLM to analyze and classify patient's response as 'yes', 'no', or 'unclear' based on clinical context.\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "    \n",
    "    analysis_prompt = PromptTemplate(\n",
    "        input_variables=[\"response\", \"question_context\"],\n",
    "        template=\"\"\"You are a skilled clinician analyzing a patient's response to a MINI interview question.\n",
    "\n",
    "Question context: {question_context}\n",
    "Patient response: \"{response}\"\n",
    "\n",
    "Analyze this response and classify it as exactly one of: 'yes', 'no', or 'unclear'\n",
    "\n",
    "Guidelines:\n",
    "- 'yes': Patient clearly indicates affirmative (agreement, presence of symptoms, positive response)\n",
    "- 'no': Patient clearly indicates negative (disagreement, absence of symptoms, negative response)  \n",
    "- 'unclear': Response is ambiguous, contradictory, or requires clarification\n",
    "\n",
    "Consider the clinical context and nuances in the patient's language. Look beyond just keywords to understand intent and meaning.\n",
    "\n",
    "Classification: \"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = analysis_prompt.format(response=response, question_context=question_context)\n",
    "        llm_response = llm.invoke(result)\n",
    "        \n",
    "        # Extract just the classification from the response\n",
    "        classification = llm_response.content.strip().lower()\n",
    "        if 'yes' in classification:\n",
    "            return 'yes'\n",
    "        elif 'no' in classification:\n",
    "            return 'no'\n",
    "        else:\n",
    "            return 'unclear'\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LLM analysis: {e}\")\n",
    "        return 'unclear'\n",
    "\n",
    "@tool\n",
    "def analyze_response_tool(response: str, question_context: str) -> str:\n",
    "    \"\"\"Tool version of analyze_response for agent use.\"\"\"\n",
    "    return analyze_response(response, question_context)\n",
    "\n",
    "def ask_clarification(original_question: str, unclear_response: str) -> str:\n",
    "    \"\"\"Ask the patient to clarify their unclear response with a more specific follow-up question.\"\"\"\n",
    "    clarification_prompts = [\n",
    "        f\"I want to make sure I understand your response correctly. When I asked: '{original_question}', you said: '{unclear_response}'. Could you please answer with a simple yes or no?\",\n",
    "        f\"Let me rephrase that question to be clearer: {original_question}. Would you say yes or no?\",\n",
    "        f\"I'd like to clarify your previous response. Can you tell me more specifically - would your answer be yes or no to: {original_question}?\"\n",
    "    ]\n",
    "    \n",
    "    import random\n",
    "    clarification = random.choice(clarification_prompts)\n",
    "    print(f\"Clinician: {clarification}\")\n",
    "    patient_response = input(\"Patient: \")\n",
    "    print(f\"Patient response: {patient_response}\")  # Print patient answer\n",
    "    return patient_response\n",
    "\n",
    "@tool\n",
    "def ask_clarification_tool(original_question: str, unclear_response: str) -> str:\n",
    "    \"\"\"Tool version of ask_clarification for agent use.\"\"\"\n",
    "    return ask_clarification(original_question, unclear_response)\n",
    "\n",
    "@tool\n",
    "def explain(current_question: str) -> str:\n",
    "    \"\"\"Clarify or rephrase the current question to help the patient understand.\"\"\"\n",
    "    explanations = {\n",
    "        \"anxiety\": \"Anxiety means feeling worried, nervous, or uneasy about something.\",\n",
    "        \"panic\": \"Panic symptoms include rapid heartbeat, sweating, trembling, or feeling like you can't breathe.\",\n",
    "        \"agoraphobia\": \"This refers to fear of being in places where escape might be difficult or help unavailable.\",\n",
    "        \"avoidance\": \"Avoidance means staying away from situations that make you uncomfortable.\"\n",
    "    }\n",
    "    \n",
    "    explanation = f\"Let me clarify: {current_question}\\n\\n\"\n",
    "    for key, value in explanations.items():\n",
    "        if key in current_question.lower():\n",
    "            explanation += f\"Note: {value}\\n\"\n",
    "    \n",
    "    return explanation\n",
    "\n",
    "@tool\n",
    "def end_module() -> str:\n",
    "    \"\"\"Signal that the module has ended either due to logic or patient response.\"\"\"\n",
    "    return \"Module assessment complete. Thank you for your responses.\"\n",
    "\n",
    "@tool\n",
    "def get_next_question(current_question_id: str, patient_answer: str, question_context: str) -> str:\n",
    "    \"\"\"Determine the next question based on current question and patient's LLM-analyzed answer.\"\"\"\n",
    "    # Check if questions dictionary exists\n",
    "    try:\n",
    "        if current_question_id not in questions:\n",
    "            return \"END_MODULE\"\n",
    "        \n",
    "        branching = questions[current_question_id].get(\"next\", {})\n",
    "        classified_answer = analyze_response(patient_answer, question_context)  # Uses the regular function\n",
    "        \n",
    "        next_q = branching.get(classified_answer, \"END_MODULE\")\n",
    "        \n",
    "        if next_q == \"END_MODULE\":\n",
    "            return \"END_MODULE\"\n",
    "        elif next_q in questions:\n",
    "            return questions[next_q][\"prompt\"]\n",
    "        else:\n",
    "            return \"END_MODULE\"\n",
    "    except NameError:\n",
    "        # If questions is not defined yet, return a placeholder\n",
    "        print(\"Warning: questions dictionary not loaded yet\")\n",
    "        return \"QUESTIONS_NOT_LOADED\"\n",
    "\n",
    "# List of tools for the agent (clean and focused)\n",
    "tools = [ask_patient, analyze_response_tool, ask_clarification_tool, explain, end_module, get_next_question]\n",
    "\n",
    "print(\"Core interview tools defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Load MINI Questions (Module E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 questions for Module E: Agoraphobia\n",
      "\n",
      "1. E1: Do you feel anxious or uneasy in places or situations where help might not be available or escape difficult if you had a panic-like or embarrassing symptom, such as being in a crowd or queue, in an open space or crossing a bridge, in an enclosed space, when alone away from home, when alone at home, or traveling in a bus, train, car, or using public transportation?\n",
      "\n",
      "2. E2: Do these situations almost always bring on fear or anxiety?\n",
      "\n",
      "Branching for E1:\n",
      "{'yes': 'E2', 'no': 'END_MODULE'}\n"
     ]
    }
   ],
   "source": [
    "# Load module data\n",
    "import json, pprint\n",
    "\n",
    "PATH = \"mini_modules/working/module_e.json\" \n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    module_data = json.load(f)\n",
    "\n",
    "mini_script = module_data[\"questions\"]     \n",
    "module_info = module_data[\"module\"]          \n",
    "\n",
    "# Prepare questions dictionary for easy access\n",
    "questions = {q[\"id\"]: q for q in mini_script}\n",
    "\n",
    "print(f\"Loaded {len(mini_script)} questions for Module {module_info['id']}: {module_info['name']}\\n\")\n",
    "\n",
    "for i, q in enumerate(mini_script[:2], start=1):\n",
    "    print(f\"{i}. {q['id']}: {q['prompt']}\\n\")\n",
    "\n",
    "# Branching logic retreival function\n",
    "def show_branches(qid: str):\n",
    "    if qid not in questions:\n",
    "        raise KeyError(f\"{qid!r} not found\")\n",
    "    next = questions[qid].get(\"next\", {})\n",
    "    print(f\"Branching for {qid}:\")\n",
    "    pprint.pp(next)\n",
    "\n",
    "# Example: inspect the first question’s branches\n",
    "show_branches(\"E1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"question\"],\n",
    "    template=\"\"\"You are an AI agent with the role of a skilled clinician trained in professional physchological diagnosis and care conducting an interview based on the MINI questionnaire. Be concise, empathetic, and professional.\n",
    "\n",
    "Previous conversation:\n",
    "{history}\n",
    "\n",
    "Next question to ask:\n",
    "{question}\n",
    "\n",
    "Present this question in a natural, conversational way. Keep your response brief and focused, ask the question prompting a yes or no response from the patient.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Class to Manage Interview State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interview tools ready: ask_patient, analyze_response, ask_clarification, explain\n"
     ]
    }
   ],
   "source": [
    "# Interview State Management\n",
    "import datetime\n",
    "\n",
    "class InterviewState:\n",
    "    def __init__(self):\n",
    "        self.current_question_id = \"E1\"\n",
    "        self.conversation_log = []\n",
    "        self.is_complete = False\n",
    "        self.interview_start_time = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_question_id = \"E1\"\n",
    "        self.conversation_log = []\n",
    "        self.is_complete = False\n",
    "        self.interview_start_time = datetime.datetime.now()\n",
    "\n",
    "# Initialize global state\n",
    "interview_state = InterviewState()\n",
    "\n",
    "# Core tools for the interview agent (keeping the essential ones)\n",
    "print(\"Interview tools ready: ask_patient, analyze_response, ask_clarification, explain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Export the Interview as a CSV Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "\n",
    "def export_results_to_csv():\n",
    "    \"\"\"Export the interview results to a CSV file.\"\"\"    \n",
    "    reports_dir = \"reports\"\n",
    "    if not os.path.exists(reports_dir):\n",
    "        os.makedirs(reports_dir)        \n",
    "    \n",
    "    # Use timestamp to name the file, guaranteed to be unique\n",
    "    if interview_state.interview_start_time:\n",
    "        timestamp = interview_state.interview_start_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    else:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    filename = f\"mini_interview_{timestamp}.csv\"\n",
    "    filepath = os.path.join(reports_dir, filename)\n",
    "        \n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Question ID', 'Question', 'Patient Response', 'Classification', 'Clarifications Needed']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        \n",
    "        for entry in interview_state.conversation_log:\n",
    "            writer.writerow({\n",
    "                'Question ID': entry['question_id'],\n",
    "                'Question': entry['question'],\n",
    "                'Patient Response': entry['patient_response'],\n",
    "                'Classification': entry['classification'],\n",
    "                'Clarifications Needed': entry.get('clarifications_needed', 0)\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nResults exported to: {filepath}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Agent-Based Interview Workflow\n",
    "def run_interview_with_agent():\n",
    "    \"\"\"Run the MINI interview with agent assistance for response analysis and clarification.\"\"\"\n",
    "    # Reset interview state\n",
    "    interview_state.reset()\n",
    "    \n",
    "    print(f\"Starting MINI Module {module_info['id']}: {module_info['name']}\")\n",
    "    print(\"Using AI Agent for intelligent response analysis and clarification\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize LLM for agent decisions\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "    \n",
    "    while not interview_state.is_complete:\n",
    "        # Get current question\n",
    "        if interview_state.current_question_id == \"END_MODULE\":\n",
    "            break\n",
    "            \n",
    "        if interview_state.current_question_id not in questions:\n",
    "            print(\"No more questions available.\")\n",
    "            break\n",
    "            \n",
    "        current_question = questions[interview_state.current_question_id]\n",
    "        \n",
    "        # Display question\n",
    "        print(f\"\\n[Question {interview_state.current_question_id}]\")\n",
    "        print(f\"Clinician: {current_question['prompt']}\")\n",
    "        \n",
    "        # Get patient response\n",
    "        patient_response = input(\"Patient: \")\n",
    "        print(f\"Patient answered: {patient_response}\")\n",
    "        \n",
    "        # Use agent to analyze response\n",
    "        classification = analyze_response(patient_response, current_question['prompt'])\n",
    "        print(f\"LLM Analysis: {classification}\")\n",
    "        \n",
    "        # If unclear, use agent for clarification\n",
    "        max_clarifications = 2\n",
    "        clarification_count = 0\n",
    "        \n",
    "        while classification == 'unclear' and clarification_count < max_clarifications:\n",
    "            clarification_count += 1\n",
    "            print(f\"\\n[Clarification {clarification_count}/{max_clarifications}]\")\n",
    "            \n",
    "            # Get clarified response\n",
    "            clarified_response = ask_clarification(current_question['prompt'], patient_response)\n",
    "            print(f\"Clarified response: {clarified_response}\")\n",
    "            \n",
    "            # Re-analyze clarified response\n",
    "            classification = analyze_response(clarified_response, current_question['prompt'])\n",
    "            print(f\"LLM Analysis of clarification: {classification}\")\n",
    "            \n",
    "            # Update patient response to the clarified version\n",
    "            patient_response = clarified_response\n",
    "        \n",
    "        # If still unclear after max attempts, default to 'no'\n",
    "        if classification == 'unclear':\n",
    "            print(f\"Still unclear after {max_clarifications} attempts. Defaulting to 'no' to continue.\")\n",
    "            classification = 'no'\n",
    "        \n",
    "        # Log the interaction\n",
    "        interview_state.conversation_log.append({\n",
    "            \"question_id\": interview_state.current_question_id,\n",
    "            \"question\": current_question['prompt'],\n",
    "            \"patient_response\": patient_response,\n",
    "            \"classification\": classification,\n",
    "            \"clarifications_needed\": clarification_count\n",
    "        })\n",
    "        \n",
    "        # Use branching logic to determine next question\n",
    "        branching = current_question.get(\"next\", {})\n",
    "        next_question_id = branching.get(classification, \"END_MODULE\")\n",
    "        \n",
    "        print(f\"Based on '{classification}' response, next: {next_question_id}\")\n",
    "        \n",
    "        if next_question_id == \"END_MODULE\":\n",
    "            interview_state.is_complete = True\n",
    "            print(\"Interview complete based on branching logic.\")\n",
    "        else:\n",
    "            interview_state.current_question_id = next_question_id\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"INTERVIEW COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nModule: {module_info['name']}\")\n",
    "    print(f\"Total questions asked: {len(interview_state.conversation_log)}\")\n",
    "    \n",
    "    print(\"\\nCONVERSATION SUMMARY:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, entry in enumerate(interview_state.conversation_log, 1):\n",
    "        print(f\"\\n{i}. Question {entry['question_id']}:\")\n",
    "        print(f\"   Clinician: {entry['question']}\")\n",
    "        print(f\"   Patient: {entry['patient_response']}\")\n",
    "        print(f\"   Classification: {entry['classification']}\")\n",
    "        if entry.get('clarifications_needed', 0) > 0:\n",
    "            print(f\"   Clarifications needed: {entry['clarifications_needed']}\")\n",
    "    \n",
    "    # Export results of the interview to CSV\n",
    "    export_results_to_csv()\n",
    "    \n",
    "    return interview_state.conversation_log\n",
    "\n",
    "# Demo function for testing LLM analysis\n",
    "def run_llm_analysis_demo():\n",
    "    \"\"\"Demonstrate the LLM-powered analysis capabilities.\"\"\"\n",
    "    print(\"=== LLM Analysis Tool Demonstration ===\")\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"yes definitely\", \"Do you feel anxious in crowded places?\"),\n",
    "        (\"not really, maybe sometimes\", \"Do you avoid certain situations?\"),\n",
    "        (\"I get nervous but I still go\", \"Do these situations cause you distress?\"),\n",
    "        (\"absolutely not\", \"Have you experienced panic attacks?\"),\n",
    "        (\"well, it depends on the day\", \"Do you feel this way consistently?\")\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting LLM-powered response analysis:\")\n",
    "    for response, question in test_cases:\n",
    "        try:\n",
    "            classification = analyze_response(response, question)\n",
    "            print(f\"'{response}' -> {classification}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing '{response}': {e}\")\n",
    "    \n",
    "    print(\"\\nLLM analysis tool working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Demo Run with Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MINI Module E: Agoraphobia\n",
      "Using AI Agent for intelligent response analysis and clarification\n",
      "======================================================================\n",
      "\n",
      "[Question E1]\n",
      "Clinician: Do you feel anxious or uneasy in places or situations where help might not be available or escape difficult if you had a panic-like or embarrassing symptom, such as being in a crowd or queue, in an open space or crossing a bridge, in an enclosed space, when alone away from home, when alone at home, or traveling in a bus, train, car, or using public transportation?\n",
      "Patient answered: yes\n",
      "LLM Analysis: yes\n",
      "Based on 'yes' response, next: E2\n",
      "\n",
      "[Question E2]\n",
      "Clinician: Do these situations almost always bring on fear or anxiety?\n",
      "Patient answered: no\n",
      "LLM Analysis: no\n",
      "Based on 'no' response, next: END_MODULE\n",
      "Interview complete based on branching logic.\n",
      "\n",
      "======================================================================\n",
      "INTERVIEW COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Module: Agoraphobia\n",
      "Total questions asked: 2\n",
      "\n",
      "CONVERSATION SUMMARY:\n",
      "--------------------------------------------------\n",
      "\n",
      "1. Question E1:\n",
      "   Clinician: Do you feel anxious or uneasy in places or situations where help might not be available or escape difficult if you had a panic-like or embarrassing symptom, such as being in a crowd or queue, in an open space or crossing a bridge, in an enclosed space, when alone away from home, when alone at home, or traveling in a bus, train, car, or using public transportation?\n",
      "   Patient: yes\n",
      "   Classification: yes\n",
      "\n",
      "2. Question E2:\n",
      "   Clinician: Do these situations almost always bring on fear or anxiety?\n",
      "   Patient: no\n",
      "   Classification: no\n",
      "\n",
      "Results exported to: reports/mini_interview_20250730_204933.csv\n"
     ]
    }
   ],
   "source": [
    "# Execute the interview (using the agent-based approach)\n",
    "results = run_interview_with_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
